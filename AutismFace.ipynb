{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"jwwwik0Hm3u5","executionInfo":{"status":"ok","timestamp":1687637222806,"user_tz":-120,"elapsed":837,"user":{"displayName":"Hong Lu","userId":"18117207304411534237"}}},"outputs":[],"source":["![ -d vision_transformer ] || git clone --depth=1 https://github.com/google-research/vision_transformer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1e1eTgonVsc"},"outputs":[],"source":["!pip install -qr vision_transformer/vit_jax/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbll2Shs4uYp"},"outputs":[],"source":["!pip install jaxlib --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCKi-FgWnXod"},"outputs":[],"source":["# Import files from repository.\n","\n","import sys\n","if './vision_transformer' not in sys.path:\n","  sys.path.append('./vision_transformer')\n","\n","%load_ext autoreload\n","%autoreload 2\n","from vit_jax import checkpoint\n","from vit_jax import models\n","from vit_jax import train\n","from vit_jax.configs import augreg as augreg_config\n","from vit_jax.configs import models as models_config"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hYLPeNCHnZar","executionInfo":{"status":"ok","timestamp":1687637279660,"user_tz":-120,"elapsed":431,"user":{"displayName":"Hong Lu","userId":"18117207304411534237"}}},"outputs":[],"source":["import glob\n","import os\n","import random\n","import shutil\n","import time\n","\n","from absl import logging\n","import pandas as pd\n","import seaborn as sns\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","pd.options.display.max_colwidth = None\n","logging.set_verbosity(logging.INFO)  # Shows logs during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGUpZUsFnbMu"},"outputs":[],"source":["# Create a new temporary workdir.\n","workdir = f'./workdirs/{int(time.time())}'\n","workdir"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mus4qWs6QHZ4"},"outputs":[],"source":["workdir"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"mxBqJi_xnc0f","executionInfo":{"status":"ok","timestamp":1687637287912,"user_tz":-120,"elapsed":353,"user":{"displayName":"Hong Lu","userId":"18117207304411534237"}}},"outputs":[],"source":["config = augreg_config.get_config('S_16')\n","config.dataset = 'drive/MyDrive/AutismFaceData'\n","config.batch_eval = 300\n","\n","# Some more parameters that you will often want to set manually.\n","# For example for VTAB we used steps={500, 2500} and lr={.001, .003, .01, .03}\n","config.base_lr = 0.01\n","# config.shuffle_buffer = 1000 #default value is 50_000\n","config.total_steps = 100\n","config.warmup_steps = 10\n","# config.accum_steps = 0  # Not needed with R+Ti/16 model.\n","config.pp['crop'] = 224"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqCdp0kfndRy"},"outputs":[],"source":["# And fine-tune on images provided\n","opt = train.train_and_evaluate(config, workdir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuJ_H3Y5ne-i"},"outputs":[],"source":["import flax\n","from flax.training import checkpoints as flax_checkpoints\n","from vit_jax import input_pipeline\n","dataset_info = input_pipeline.get_dataset_info(config.dataset, 'train')\n","_, ds_test = input_pipeline.get_datasets(config)\n","model_config = models_config.AUGREG_CONFIGS['S_16']\n","cp_path = \"./workdirs/1687423517/checkpoint_100\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGOY_lRwngnb"},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","# Get model instance - no weights are initialized yet.\n","model = models.VisionTransformer(\n","    num_classes=dataset_info['num_classes'], **model_config)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJKWwCZpDrCx"},"outputs":[],"source":["# Get a single example from dataset for inference.\n","batch = next(iter(ds_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjcTQXSVGRiR"},"outputs":[],"source":["def init_model():\n","    return model.init(\n","        jax.random.PRNGKey(0),\n","        # Discard the \"num_local_devices\" dimension for initialization.\n","        jnp.ones(batch['image'].shape[1:], batch['image'].dtype.name),\n","        train=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mo8KXX-bGUvu"},"outputs":[],"source":["import optax\n","from vit_jax import utils\n","# Load a checkpoint from cloud - for large checkpoints this can take a while...\n","variables = jax.jit(init_model, backend='cpu')()\n","params=variables['params']\n","lr_fn = utils.create_learning_rate_schedule(100, config.base_lr,\n","                                              config.decay_type,\n","                                              config.warmup_steps)\n","tx = optax.chain(\n","    optax.clip_by_global_norm(config.grad_norm_clip),\n","    optax.sgd(\n","        learning_rate=lr_fn,\n","        momentum=0.9,\n","        accumulator_dtype='bfloat16',\n","    ),\n",")\n","opt_state = tx.init(params)\n","initial_step = 1\n","params, opt_state, initial_step = flax_checkpoints.restore_checkpoint(\n","      cp_path, (params, opt_state, initial_step))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6bIoKrtPPE3"},"outputs":[],"source":["def pp(img, sz):\n","  \"\"\"Simple image preprocessing.\"\"\"\n","  img = tf.cast(img, float)\n","  img = tf.image.resize(img, [sz, sz])\n","  return img\n","resolution = 128\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joMe8BtY5Q07"},"outputs":[],"source":["def show_img(img, ax=None, title=None):\n","  \"\"\"Shows a single image.\"\"\"\n","  if ax is None:\n","    ax = plt.gca()\n","  ax.imshow(img[...])\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  if title:\n","    ax.set_title(title)\n","\n","def show_img_grid(imgs, titles):\n","  \"\"\"Shows a grid of images.\"\"\"\n","  n = int(np.ceil(len(imgs)**.5))\n","  _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n","  for i, (img, title) in enumerate(zip(imgs, titles)):\n","    img = (img + 1) / 2  # Denormalize\n","    show_img(img, axs[i // n][i % n], title)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODp9z0IQXx54"},"outputs":[],"source":["import numpy as np\n","correct_num = 0\n","total_num = 0\n","images = batch['image'][0]\n","labels = batch['label'][0]\n","mis_images = []\n","mis_labeled = []\n","for i in range(images.shape[0]):\n","  img = images[i]\n","  label = labels[i]\n","  total_num += 1\n","  logits, = model.apply({'params': params}, pp(img, 224).numpy()[None], train=False)\n","  predict_label = np.argmax(np.exp(logits) / np.sum(np.exp(logits)))\n","\n","  if predict_label == np.argmax(label):\n","    correct_num += 1\n","  else:\n","    mis_images.append(img)\n","    mis_labeled.append('autistic' if np.argmax(label) == 0 else 'non-autistic')\n","\n","print(correct_num)\n","print(total_num)\n","show_img_grid(mis_images, mis_labeled)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFRzUAiXnkRX"},"outputs":[],"source":["# Inference on batch with single example.\n","logits, = model.apply({'params': params}, pp(batch['image'][0][0], 250).numpy()[None], train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mh_QcVwwnmfk"},"outputs":[],"source":["# Plot logits (you can use tf.nn.softmax() to show probabilities instead).\n","plt.figure(figsize=(10, 4))\n","plt.bar(list(map(dataset_info['int2str'], range(len(logits)))), logits)\n","plt.xticks(rotation=90);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uy5HkPXnP_pT"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1a8SefuLk5vXH-ZrfyMN-U_ooEMAanbX5","authorship_tag":"ABX9TyO9FIx4qUMI/eBIFB+ephrO"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}